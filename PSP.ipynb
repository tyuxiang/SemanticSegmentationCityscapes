{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets.cityscapes import setupDatasetsAndLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA check\n",
    "CUDA = True\n",
    "device = \"cuda\" if (torch.cuda.is_available() and CUDA) else \"cpu\"\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "# device = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image name: ('ulm_000036_000019_leftImg8bit', 'ulm_000039_000019_leftImg8bit', 'ulm_000083_000019_leftImg8bit', 'ulm_000005_000019_leftImg8bit', 'ulm_000000_000019_leftImg8bit', 'ulm_000066_000019_leftImg8bit', 'ulm_000082_000019_leftImg8bit', 'ulm_000072_000019_leftImg8bit')\n",
      "image tensor size: torch.Size([8, 3, 224, 448])\n",
      "annotated output tensor size: torch.Size([8, 224, 448])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAADKCAYAAABe4wDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xcdZ3/8ddnLplc27Rp0qZp2qZpmt7ozVJBBAEBEdSiu7hFdPk9ZMXl6oVdrivCunXRXRRdBQVRcQGR9YIoIFcBgZbeW9qm6S29pJe0SdvcM5OZ8/n9MdN22uYySWYyk+nn+XjkkZkz5/LJ6cy73/me7zlHVBVjjDHpxZXsAowxxsSfhbsxxqQhC3djjElDFu7GGJOGLNyNMSYNWbgbY0waSli4i8ilIlItIltF5I5EbccYY8ypJBHj3EXEDWwGLgZqgeXAVaq6Me4bM8YYc4pEtdwXAFtVdbuqBoCngYUJ2pYxxpiTeBK03hJgd9TzWuCD3c08YqRLC0oyyXc5CSrHGHO6CeHQEMrEjUNzKDPZ5cRVvqeNfJfDynX+elUt7GqeRIW7dDHthP4fEbkOuA5g1FgvoX/6Fg0JKsYYc/rxtAlTXmgnmOPhwNwMSn+wClyRzgrpKqKGjqbLz+Cub/+ST5Wv39ndPIkK91qgNOr5OGBv9Ayq+gjwCMD82Zl6yIHJt69IUDnGmNNVBlDsPwMNBqm570zKn6hHd+5BsrMg0ImqIkMs7Ic9/z4v/utsYH238ySqz305UCEiZSKSASwCnkvQtowxpkfud94HYNK3VuNsqUEDAbZ9bQqMHoWMGwPjxqCqpNOFFBMS7qoaBG4CXgKqgGdUdUMitmWMMbHSYPDY47JvLsep2UVgdB6B0bm4hg+j+j+n03HWlLQI+UR1y6CqLwAvJGr9xhgTD5631wHgZGVR+pKStbWeqh9VUPnljUhONng8aFv7kOu6sTNUjTEG0PZ2sl5cRWhXLZXXb0KDQbbcVsmenxZAZyedc8shwztkWvUJa7kbY8xQpYEAAOV3Lw8/Bw5PyaRok5v2D09l1+Uupt1fizY1J7HKnlnL3RhjYjDqF8txGg6RvaWe0peUUGE+o16Cpkumgc+XcgdkLdyNMaYPnB27yXpxFbxfzcFLlLw/r6XqOxPQQIDgvMng9aRE0Fu4G2NMP2l7OxoMUnndGgiFaJgWPhO2/bypVH97OmRlJi3krc/dGGPipOhny3GA7C2HKHUVoM0tHPjCbHL3hsh+owpXTjbqD0AodMqy8T6ZysLdGGPizNm+k6ztO1Gg8JerANDpk8n5n/3U3V+BO+DgW1INgIigqgQWTKGjwEteTSuyaceAg966ZYwxJpFCofDP+9U0nd9I1ktrqJ+ZgXb42fOlM9hxy0xwHOrP8LH/bNj8+RycaRMHvFlruRtjzGBSh7E/XAbAmPfacLwuNNBJ8Y/C07YtPpPD03MZtWlgm7FwN8aYJHG9t/6U7pOcPcK4L26h491CGucUkr98H9pwuO/rjk+Jxhhj4mHMQ8tou/AwuF3sPxtCo4ZR/e/TOXz59D6NvLFwN8aYFORsqaHiX1fAmmpKXwmR//4RNj0wFbweiCHkrVvGGGNSmTpkvrQaBaZ+NQMnEECmT2ZS1rIeF7OWuzHGDBFHr3lTfW0+Xx+5vcd5LdyNMSYNWbgbY0wasnA3xpg0ZOFujDFpyMLdGGPSkIW7McakIQt3Y4xJQxbuxhiThizcjTEmDVm4G2NMGrJwN8aYNGThbowxacjC3Rhj0pCFuzHGpCELd2OMSUMDulmHiOwAmoEQEFTV+SIyEvgNMBHYAXxWVft+A0BjjDH9Fo+W+wWqOkdV50ee3wG8pqoVwGuR58YYYwZRIrplFgKPRx4/DlyRgG0YY4zpwUDDXYGXRWSliFwXmTZaVfcBRH4XdbWgiFwnIitEZMXBhtAAyzDGGBNtoDfIPkdV94pIEfCKiGyKdUFVfQR4BGD+7Ew9NMBCjDHGHDeglruq7o38PgD8AVgA1IlIMUDk94GBFmmMMaZv+h3uIpIjInlHHwOXAOuB54BrIrNdA/xxoEUaY4zpm4F0y4wG/iAiR9fzlKr+RUSWA8+IyLXALuDKgZdpjDGmL/od7qq6HZjdxfQG4KMDKcoYY8zA2BmqxhiThizcjTEmDVm4G2NMGrJwN8aYNGThbowxacjC3Rhj0pCFuzHGpCELd2OMSUMW7sYYk4Ys3I0xJg1ZuBtjTBqycDfGmDRk4W6MMWnIwt0YY9KQhbsxxqQhC3djjElDFu7GGJOGLNyNMSYNWbgbY0wasnA3xpg01O8bZBtjzFC04xsLCOQ7AHibhbL7Vp4yj7toFFW3jT/2vGCtUPCr5YNWYzxYuBtjTg9uN+J2M3p5kGCWhCcFHFw5WafO6zgUv63HnubsaWfrd+ZT+f3dhPbXDVbFA2Lhbow5LbR8cg7+YS4K/7AJXHL8Bbf7lHk1EGD461uOT3CUKd/14rS0grhAnUGoeGCsz90Yk/Zc+cNpLXKfGuwxr0AgGEQyMui4eDbOB2fGv8g4s3A3xqS9zT8spWBjR/+CPZpLyNrTgqeuMT6FJZB1yxhj0tvcaVTe2YC2tMZldVJbB44iWVlohz9lu2is5W6MSWvbvuYmUFoQ35W6BHG7Bv5NIIGs5W6MSWvl3wvi3n8Y7X3WvnMSsta46DXcReTnwCeAA6o6MzJtJPAbYCKwA/isqh6OvHYncC0QAm5R1ZcSUrkxxvTCPW4s2t6JtrbFv5UtLlxTymiuzCf6fw5va4iM19fEd1v9EEvL/ZfAj4BfRU27A3hNVe8XkTsiz28XkenAImAGMBZ4VUSmqGoovmUbYwaba9IENt1YSMWTrew7N4+xP1qJeDxsuXcWToZS+YsmdP3mAW2j6e/ns//sgdc64cUQvtfWEigtIDDcS87eAwNfaRdaJg9nz/knTvPVZzDxrd6jVVVjavmL18OWb83CiVrlBR9c3+tyvVagqm+JyMSTJi8Ezo88fhx4A7g9Mv1pVfUDNSKyFVgALOm1EmP6QTweJCMj5vk1GEQDgT5vJ3juLIr+o6bPy3Xn8A1j0I3bBryezY+ewZQv9/5B741I761aaWqh5M1ReOqbKVydgXg84HIx9p0Q6hJcDU2E3O5jgRU6awZ1Z2Yx9senngHa3faGbWnGHcg9Zbr2sdGdtbsJzfDiWbMVr8eTmL5xl5D71ham/jUUDuqjHKfLsfMni7kil4vitxWNOkK6+c0Z/Ne/7wO2drtYf/vcR6vqPgBV3SciRZHpJcDSqPlqI9OMOabj4rmEMrs/lj9szX5Cu/cA4CyYgXvtVtTvPz6DHF/24BfmMvn/VR977uqlZ3Xdn2ZS+sOTvjLHEGzeFVs4sjDjhG0PSEctkuEd8Gqm3lqDegbn0JnT3ELuKxtxgIyDDeAK74uc16rCrztOOPAjvGu3MW69u0/1yeZd5A6s8Q9EeklcrsQf9HRJuAP6hGnxH6eS++rGEyeEQuy8c1SPy8T7XdHVXuzy0yYi1wHXAYwv8aTyQWdzksOfO5ODH4jtQNKIDULh46tOmLbvHA+deZHl5dT1+A4X4K07CMDBOTmM3Z6LE90yigrjwqfW0fjUSSvo4cNVyvsxtaq6ov6+t/h7FIcQ0EBnQsKkX06qQ1UhGEyd+hLFJRBKvQOr/Q33OhEpjrTai4GjHVq1QGnUfOOAvV2tQFUfAR4BmD87UxtcdP/12nFO/NrTT5t/OjP8FXYAR7i3Lf4AZ56zacC1DNTSrWVMuXZdTPO6cnPIfC6DTHewy9c3PTGVop+Fvzrvun0+419shPXHv+6JyAkf0PxNLXhbs2Padvb+jhNacwCT/vOkurv68EcCeMz/vo8T9dyYlOMokt3F9Wn6IhgMN1pifZ/7/Zz6leFE/Q3354BrgPsjv/8YNf0pEfke4QOqFcCymNfa3R8mEnv/VA/i8RW24jvVHHHF4QvPAE98mKo70Bj7mrUzSMdnPHREn9YQ1b0wpnMjGukimPjjKgiFTt1PUQHsqtpBXlUfik33lptJLKd/n5W2C2bgaQvhWxt1rMTRUz97EtV1c7Thd3JXwskNwqj5Ax+YjN5dT0Fm/0+S2vinqbSWBTlzZmzHYar/bwYTWdXjPLEMhfw14YOno0SkFvgm4VB/RkSuBXYBVwKo6gYReQbYCASBG2MdKdPjAZM4hUM8vsJqoBPojEs9A9aH1myvXQqR/aLB4AnPjemTXoJ41y2zaS/pORIkIEz5t3Vo6Ph8R66cy4Gzjgesq93F5LtWnhDUelIA7zvHTdHcenxXaTiMHaXuyqmMeWEXwb37AfAUjWLv35VT/FQVFOSz/6IxqEsY++IetKk58jcpbWdPZtflLrxHXEz69rpwL4NLQB0OVfoY85kjNDY1xbSLulLCuwCcfFGD9oUL2Huui8l3r6b1sjns/YiQXetieG2Iy0es4ac9rDOW0TJXdfPSR7uZfzGwuLf1GmOSICp8xe0Grwft8IcfZ2VG5lG0rS382OVCMo9OD4XHiwOhuZUUPbATgKU1ZUz+YuSAXySQxefD90Iuud6oA+HA/iehcF3P/wFICJz2dojqih25+jCZh4cdnycYQjt7brBMuncVzJqC+vcjebkcuWASbWME59BhcMJ1OkcaGb2sGQ0E2PYfeUz6r3BA6+EjSGbmsUZR9o5Gxr8wHHd7EKe9A5fLFT4g7nZT+NOlhOLQbRz+w4S6m89mRHWArDv2smeNi5I3wweqc2uaKXbl4TviJ6OhnY0dJUD3I6UkHn3ZAzV/dqbW/9NiKv9tQ7JLMWboOKmV3HrRDEK+49+4cvZ04F4dHnoiZaU0Ts8HIHd3O7K6GqksY8cVI5nw7WXI9MnkPRQ+iL1m9zjKrg6HhuuMKeT9OHxIbdXO8ZRfvTq8chFcPh8QPnB6wmimCNfR/xSiS/b7TwjtwbL5sflUfnlNuHXvdP3NQXy+Y3+HZ+J4tn6phLJ7V+IqK2X754uYcE/UiG4R3FPKqfmHIsb/+7vxKVIEPngGtRfmMuFH68Hng2Dw+LfpaKEQ5W+GeHj+kytVdX6Xq7NwNybFRIe2Kgevns3oV/bQUV5Izac9eJpd4VYpsP2b8wjmOUz6fSe1F/jozNVjA5AK1gr5/xsemRy8YB47Lw8fVxm1CoY/uRSTfHU3f4hgDox7YEV42KbbHVuXaAzhbteWMSaeTmpNS3Z2uG820IkGAuHuD58P1KH5vMlM+JfjY/SrH51GwRMraVk4l4lfr6amaSS5l+1k1IomnIMNZIYcxr9YjLsjeKyFWfpagFCmC9/WA0z4a223ZXleX0n564n5k01sXJmZOB0diM+HuN3U/eNsQj4oWhUId/HE+TiXhbsxPTkprDvOnY4EHTIaAzSX5ZDZ0EnGkio6zptBIM/FsOpGtLrm2KiMrY9WMHf8brb/cioFjy3Ff9l8iu7czt6W4eR+YiUNLxz/CI4MLEOdEDm/X0HDn73k6R7UCaGrN6CA09qKb/eJAe55bSUewqMXTGpwDxtGywVTyVu5h9ZZY3G8Qt6afWy8ZzSVX15D9fdn4x3hZ/L1G8PHN1QTMoDBwt2YowEe6aJs/OQs6s6Gyns24gSD7P76B2gfG6Lkr3Ck3I0rALl7PdR9EHJ3+hj9ejv1s7y0j3bwtuThW3e8/3nS59bQCBRErsDhe345jc9DDg0odN2f6oRwOuxyTENJzbfPJpStoDDlsSPUXuhicn0he873EMp0mHxwJFNvqQKfj6m3h8cRJ7pD3MLdpA/HCY/u8GVAyIEML9rWHj6JyuMBJ8Teq6eSv7WT3Ntq2flCGSXfX4Fz5jSK/nsndXeW4XprDfnrGshoyifU0gpOiJK32gjke8necojs3x8fh5z7f8c3Pfa7cTqoZoYU8Xio/uE8bj//OZ74xicQBdlTR+VduwGYvC6qRT7Iw4st3M3QEN09Ul6KtAdoPqMQxyMMX1WHtLRx5NyJ5K+sY+Ndoxi53Evp57YT+IyX/VdWMvnzm1mxfSJTvrgMdZTQ615KggfRYBBZso6GC324/GtAlVDVFnxRJ2nJO2vw0dv5gOZ0dOjqMzl/3nqe+/TZ5O0NDwc9el2bZLNwN6kjqntk9y1zGLWuk12XuZj2HzsIHT5C3Zc+QEcBjHmvk4wjfurmu3B8St72HNzBEHVnCXlbc5jyTysAaI+c4VH4kyU0/gQqIl0hwIndHqo4HR2D93ea9CDCyKdXsf+3HnDVJbuaU1i4m/iLhPSxkSKAtrWD4yCZPhA5dj/LI38ooXx4AzUPTmX42npG/qKeHQ9UUvx2Gxl7DjNeiggdPoL6/Yx+r5lgjhf3G+FhgGWR0XxK+IBi+a27Et6PaQxwfJx/rEMXk8DC3fRNF6eXt10wg4zmTjzLqxGfj8aLpjB8XQO13/EydVT4BJjWfywguGM3NXfPImv6EcZcEe73GH5FLQ0i5AaWE3JCNFyYSU5gBTghgoCvZuexwNbl72OXDzNJ53LjijRSUjXYwcLddOdo6zs3h73/MJmMJuXwdJh8z2oaFs3l8DQov3s56igH53rI3u+m4K123FmZ1J0lZB0cRvEVq6KuldEAQNldJ963Rf3+E1rb1j1iUtoQCXawcD+9HA1sryd8Ik17JEh9PsTjJu+3ATziUH9zCe3FOZTetZn6mwsZvbQZV0cnuXtzcTo6KFh1mNy9uceG8ZV+6/hIkVDDIcpvtbMfTXoSr2dIBDtYuKefSID7z55K1vYGjswfQ/67u8HtwsnPRTduw3/2VEbdu4Omr5QSGJHJqHt30HxTIY0f3Q6A+jeSKS4aXvWi/o3hA46AN3IZdmfdpmOPjTltnDUL15bayJVhU5+F+1DlOKBK7Q1zKHmzmc3XZFP5aBOIUH1tHq4OF5OfyqHugzB8TSaa6aV9XC6+tQG8r66k8VWABrxw7PEJ1E6kMeao0AXzqFmYwdT/zrJwN3109EClyxW+q0swGL4Ua3Y2dAaoeqCCc6ZtZe2z0xn38Pt4X8hjx3OTKH63FdeuOsa/OAHZEz54Of4vOWQv3RLuIll9fHy2z1rbxvSLr6aeyoczcBr7f832wWbhPpiOBvjk8Uibn+YzCsmubcO1fhv+D00j42/rERH8T46g7pVxjHtwJcGnM6l7oYLKG1bSIEJJcBlOMEjnxzoZG1yGBoOEAN/zB46H+PMNdsKNMXEU3LGL7fefzZSH/Tj1h6zP/bQV6TJpu3Amuy8VfPVuJt6/in3XzcOfD6OXd+Jr8FB3potRGTnkrfDTMNPHmFfDI0c8F+2ihPCYbddHd1PM7vB1SKI3YaNKjBlUk+5YAiVjOXjVLAp/0/1NMlKFhXusHOf4KJPInWok04d2+CEUYvN/ziG3rJHiK7ex5bvzyJrQzPjbDjJeR+Fp7cTp6GD00mZC2VEn4UTdXXbMg3ZtEmNSnXOkkYJ1LYgIqXAvjJ5YuEc7+fKuH55G5t82Epw3hUB+Bv7hLsZfv4VN9UUUf2Yz2++ZxZQHawju20/FbavAJWhn4NjjkN+Pb2vUzXmX2Uk4xgxlTmsrjRU5eMZPI+/F9SndPXP6hnskyA9eNYvCFY3UnT2cop+vouYb88jdBaN+toyGmRkUv9zOkYosWsYL4+97l8YnoTgysmTS7UuOXUc7+n6Ovd3b0RgzdA1/cmn4SqOR2wymqtT9b6e/jl3XJCv8I3Lsp/ZXpRS85Kb+72fiGpFPwSteWksEqa2jaFkT6vcz7vUAhcubwAlR/MC7oMrIXyxh/H3WbWKMCV/mVzIykl1Gr4Zuyz2qC8U5oxzPzgNoThaNc4sYvrae7McacYnSct0YWsvz6RjhZtxVq2kgfMebYOQ6JuMD7xFyQlAfbo17Xl9pF58yxnRrz1cX4G1VRv8qtccWp364R0LcOaOctrFZHK5wU/o/azj8mVnUzxYm37WSprJsRtb5cPJzqDtLyF/qp/nc+sgKGsjcCJnAyZe8shEnxpi+KlrlxxU69QJ6qSZlwl2U8IiUjAx2XTeNOQs38s76CipvWE31T+cw/d795B3ykbU/F6e9nZGrD5N1MA/tDDDs10vDfd81UL7C7idpjEkc79/eD1+6OoUPpkIqhXvkrJuqxRVUfmUFDT9wURlaiwaDVP7zOoKRg5SyOTxfaEM1GRuSVKwx5rR15B/m4fYrw/6yMdml9Chlwh0NDzOacsOy8Ak7UZdvsNEnxphUMfyJpYjPF74vbwq33lO3MmOMSVFbvz0XV9GoZJfRIwt3Y4zpo7Jn/WhzS7LL6FGv4S4iPxeRAyKyPmravSKyR0TWRH4ui3rtThHZKiLVIvKxRBVujDHJcnhqJttvrkx2GT2KpeX+S+DSLqZ/X1XnRH5eABCR6cAiYEZkmYdExM64N8aklYJHl7DgktS+eFiv4a6qbwGHYlzfQuBpVfWrag2wFVgwgPqMMSYl7b53Spc3jE8VA+lzv0lE1kW6bUZEppUAu6PmqY1MM8aYtNIwMwNS+MqQ/Q33h4FyYA6wD3ggMl26mLfLv15ErhORFSKy4mCD3VrCGDO0FH9vCZKVibhTc1xKv6pS1TpVDamqAzzK8a6XWqA0atZxwN5u1vGIqs5X1fmFBdYtb4wZWtyTy8j9o9B00dRkl9KlfoW7iBRHPf00cPTIwnPAIhHxiUgZUAEsO3l5Y4wZ6kJbttP44QZai9y4huUlu5xT9HqGqoj8GjgfGCUitcA3gfNFZA7hLpcdwJcBVHWDiDwDbCR8iZcbVdX6XIwx6etjh9CXfdDUnOxKTtBruKvqVV1MfqyH+RcDiwdSlDHGDBWj/3475OYQmjkJ9/rtyS7nmNQ8EmCMMUOEdgZwykvYfpMrpYZGWrgbY8wASfVOKr7bgXhT51qMFu7GGDNATnMzztoqDj4xGhmdGhcUs3A3xpg4cZ4rgCNNKdE9Y+FujDFxMuqRJTB6FNvvnJX0k5ss3I0xJo5CGzeTtxN2Xj8DyfAmrQ4Ld2OMiSdVCh5dgu+IUnPT1KR10Vi4G2NMAhQ+vITSV9sgFEpKwFu4G2NMgsg7a3D8fjQYHPSAt3A3xphEUkUDATTQOagBb+FujDGJpop2BgCQzMxB2aSFuzHGDJKDn5vFV5e8gSt/eMK3lTrnyhpjTJoreHQJt2d8icbbQhQuK2Xks4m7D6u13I0xZhAV/fhdJjzvMHLdkYSOg7dwN8aYQZbxl+U4a6vYdF8FrRdOS8iBVgt3Y4xJkopbVtA4yRO+0XacA97C3RhjksUJMeb77+J0+BGvB/FlxG3VFu7GGJNsToiq+yuZ+GJr3FrwFu7GGJMCptywkneenEfLJTPRqRMHvD4bCmnMIBBvRo936XHa2gaxGpOSnBBjHnwX95RypLUdJxQCEXD1rw1u4W5MAonHQ/vH59Fy3RFen/t4l/NUd3r4xlXXwtJ1g1ydSUWhzdsAcBcW4owvQjbt6FfAW7gbkyDNi85i30cctn7qJ7jFBWR1Od8CH3zisTd59paL8Ly2cnCLNKkrP4+Wibk0fWQ2435RFb74WB9Yn7sxCdB+xQK+tfhRahY+Egn2nt08YidffOhZeG0c4vMNQoUm1YW2bCfnd+8xemU7VQ+U9/lAq4W7MXHW8ckF/OlHP+CjWaE+LXd1XgMvTfszua/m4Zk0MTHFmSHH9eZqKq9/H8pLcY3Mj325BNZkzGnp4sVvMdzVdRdMLH5b/io7rxwbx4rMUKd+P61lw6j6egm7bzwjpmWsz92YOBJvBi7aB7wex0N4pITqwIsyaSHrj8uYfHA26hK0MwhIj/Nby92YOKr+4RxuL6ga8HpW3/ADXGdUxqEik07k3bW43l6Dq2IiE7Lqe5zXwt2YeHJpTAdQe+MTb7jlbkwXqr80kn8dua3HeSzcjYmTwMfm87OLfp7sMowBYgh3ESkVkb+KSJWIbBCRr0SmjxSRV0RkS+T3iKhl7hSRrSJSLSIfS+QfYEyqEIVOtcNYJjXE0nIPAreq6jTgLOBGEZkO3AG8pqoVwGuR50ReWwTMAC4FHhIRdyKKNyaVeF9ewfWv/WNc1nVx1Sdx1R2Ky7rM6anXcFfVfaq6KvK4GagCSoCFwNHzqR8Hrog8Xgg8rap+Va0BtgIL4l24Malo2g+b+GVT0YDXs+/lUoL76+JQkTld9anPXUQmAnOB94DRqroPwv8BAEff0SXA7qjFaiPTTl7XdSKyQkRWHGzo28kexqSq0IZq/mvDxQNaxzJ/Jzn7bAikGZiYw11EcoHfAV9V1aaeZu1i2invVFV9RFXnq+r8wgLrtTHpY8KX99Op/W+w3LTxc+T/akkcKzKno5jCXUS8hIP9SVX9fWRynYgUR14vBg5EptcCpVGLjwP2xqdcY4aGRqejX8u90+Ew8pt2bRkzcLGMlhHgMaBKVb8X9dJzwDWRx9cAf4yavkhEfCJSBlQAy+JXsjGpLVTfwN/d8DWu3fVhaoMtMS2z0h/g2l0fZvEnr0JXrE9wheZ0EEvL/RzgC8CFIrIm8nMZcD9wsYhsAS6OPEdVNwDPABuBvwA3qg7gO6oxQ1Dmn5ZRe1YLlz50G3OXL+p1/nfbKthxdyWhDdWDUJ05HfQ6KFdV36b7ixh8tJtlFgOLB1CXMYNKPB4kI/abE+/82hzOv2JVr/OVs4oZOXt6ne/mETsZ+dCzvNNUEXMNR7385hwq7unbjT6cDj841uZKZ3bGhUk77vzhtHykb9dl2b/Iz/JzfxLz/JnyZvgSAXF0dV4DV+c19Hm5zqveoW1RoE/LzH/y6xS/G3u4572304ZmDjEW7mZI6LzoA+z8RGxhKkUdbD3/kX5spf+X6U0mr7gZLn2rfcsXHg53tsZo7vJFNNWU9TiPhGDyXatRv79PtZjEsHA3CdfbzaHrF83mzOtX97iO84b9nkV5h+NdmonR6jOfhjN7niekDreev4CAk9nr+v72m3mMe2hNTNvWUMj+w+gHC3cTF57ScTTP6/oGE75b9vHbyme6XdbLG2S7Yu/vNqnJLS4eLF4R07z+r/2Njq/Gdk/Q2/ZeSNXi7k9ydwUU34vLY1rX6cTC3cSk8eqzONhDy23M1AO8M6unrpCh2eVhEsMn3piPWfx03BJ4uD/FyQAAAAXrSURBVPuTug6EWjn7d7d2+/rEP3Weljcet3A/jfTWPdJ6yUxm3t31qIvPjnyE87P6doNeYwZDkTuHbZ/t/mD4kx8v6HYU0o5rxqM7ak99wXFwOvp3IlqqsHBPM+7CQlo+1PWBr0PXtPLOgp91u6x1j5h01NMopJaX3yB06tVReLtjBN+5tesrfGbvakVXb4hrjYlg4T7EtC9cwJ4Luj/3zDeuhY0fsu4RY2KR6+r64O/l2R1c/nDXn6NvHpzBE389t8vXJj/VCsvej1t9A2HhniTi8yHuUy+YJh4P2c9nUJTZ9Wnrl+Y/wady2hJdnjGmG/cVbuC+z3bdcv/JJSWsay09YdrrL86l7P61J844CN0+Fu4J5J5cRsuMwi5fq7hzI98b90qXrw13WevamKHon/P3QP6JZyR3XvsObV888SSzXzVO5f/uOvEmdeJA5vMr43bmsKgm/7rR82dn6tRHruad96Ynu5S4qpy9ixcqX0h2GcaYIaBTQ1Q+ewPS2fuN0c87awO/GP833MVbV6rq/K7mSZlwX/ZSae8zGmOMOSblw11EDgKtQH2yaznJKKymWKViXalYE6RmXVZT7FKprgmq2mXfb0qEO4CIrOjuf6BksZpil4p1pWJNkJp1WU2xS9W6Ttane6gaY4wZGizcjTEmDaVSuPfnGq2JZjXFLhXrSsWaIDXrsppil6p1nSBl+tyNMcbETyq13I0xxsRJ0sNdRC4VkWoR2SoidyS5lh0i8n7kJuArItNGisgrIrIl8ntEgmv4uYgcEJH1UdO6rUFE7ozsu2oR+VjXa01ITfeKyJ6Tbpo+mDWVishfRaRKRDaIyFci05O9r7qrK2n7S0QyRWSZiKyN1HRfZHrS9lUPNSX1fRW1LbeIrBaRP0eeJ/V91S+qmrQfwA1sAyYBGcBaYHoS69kBjDpp2neBOyKP7wC+k+AazgPmAet7qwGYHtlnPqAssi/dg1TTvcC/dDHvYNVUDMyLPM4DNke2nex91V1dSdtfhG9wnxt57AXeA85K5r7qoaakvq+itvd14Cngz5HnSX1f9ecn2S33BcBWVd2uqgHgaWBhkms62ULg8cjjx4ErErkxVX0LOBRjDQuBp1XVr6o1wFbC+3QwaurOYNW0T1VXRR43A1VACcnfV93V1Z2E16VhR69E5438KEncVz3U1J1B+fcDEJFxwOVA9PWxk/q+6o9kh3sJsDvqeS09fxASTYGXRWSliFwXmTZaVfdB+IMLFCWhru5qSPb+u0lE1kW6bY5+TR30mkRkIjCXcOsvZfbVSXVBEvdXpJthDXAAeEVVk76vuqkJkv++ehC4DYi+O03KvK9ilexw7+oKOckcvnOOqs4DPg7cKCLnJbGWWCRz/z0MlANzgH3AA8moSURygd8BX1XVpp5m7WLaYNaV1P2lqiFVnQOMAxaIyMweZk9mTUndTyLyCeCAqsZ6X75Uy7Bjkh3utUD0FcPGAXuTVAuqujfy+wDwB8Jfr+pEpBgg8vtAEkrrroak7T9VrYt8OB3gUY5/FR20mkTESzhAn1TV30cmJ31fdVVXKuyvSB1HgDeAS0mBfXVyTSmwn84BPiUiOwh3E18oIk+QIvuqL5Id7suBChEpE5EMYBHwXDIKEZEcEck7+hi4BFgfqeeayGzXAH9MQnnd1fAcsEhEfCJSBlQAywajoKNv9IhPE95Xg1aTiAjwGFClqt+Leimp+6q7upK5v0SkUETyI4+zgIuATSRxX3VXU7LfV6p6p6qOU9WJhPPodVX9PCn4GexVso/oApcRHlGwDbg7iXVMInzUey2w4WgtQAHwGrAl8ntkguv4NeGvo52EWwXX9lQDcHdk31UDHx/Emv4XeB9YR/gNXjzINX2Y8NffdcCayM9lKbCvuqsrafsLmAWsjmx7PXBPb+/tJNaU1PfVSTWez/HRMkl9X/Xnx85QNcaYNJTsbhljjDEJYOFujDFpyMLdGGPSkIW7McakIQt3Y4xJQxbuxhiThizcjTEmDVm4G2NMGvr/HWiW5iTAQQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set, val_set, test_set, train_loader, val_loader, test_loader = setupDatasetsAndLoaders('./data', batch_size=8)\n",
    "\n",
    "for sample in train_loader:\n",
    "    imgSeq, annotatedOutput, imgName = sample\n",
    "\n",
    "    print(f'image name: {imgName}')\n",
    "    print(f'image tensor size: {imgSeq[-1].shape}')\n",
    "    print(f'annotated output tensor size: {annotatedOutput.shape}')\n",
    "\n",
    "    # plt.imshow(imgSeq[-1][0].permute(1, 2, 0))\n",
    "#     print(annotatedOutput[0,200:400,0:200])\n",
    "    plt.imshow(annotatedOutput[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AutoEncoder Model for MNIST\n",
    "class MNIST_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=19,pool_scales=(1,2,3,6)):\n",
    "        \n",
    "        # Init from nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder part will be a simple sequence of Conv2d+ReLU.\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(3, 16, 3, stride = 2, padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(16, 32, 3, stride = 2, padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, 3, stride = 2, padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(64, 128, 3, stride = 2, padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(128, 256, 3, stride = 2, padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(256, 256, 3, stride = 2, padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(256, 512, 3),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(512, num_classes, 1))\n",
    "        \n",
    "        \n",
    "        # Decoder part will be a simple sequence of TransposeConv2d+ReLU.\n",
    "        # Finish with Sigmoid\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(num_classes, num_classes, 3),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(num_classes, num_classes, 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(num_classes, num_classes, 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(num_classes, num_classes, 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(num_classes, num_classes, 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(num_classes, num_classes, 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(num_classes, num_classes, 3, stride = 2, padding = 1, output_padding = 1),\n",
    "                                     nn.ReLU())\n",
    "        \n",
    "        \n",
    "        #PSP variables\n",
    "        self.pool_scale = pool_scales\n",
    "        dim = 16\n",
    "\n",
    "        self.psp = []\n",
    "        for scale in pool_scales:\n",
    "            self.psp.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(scale),\n",
    "                nn.Conv2d(num_classes, dim, kernel_size=1, bias=False),\n",
    "#                 nn.BatchNorm2d(dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.psp = nn.ModuleList(self.psp)\n",
    "\n",
    "        self.conv_last = nn.Sequential(\n",
    "#             nn.Conv2d(num_classes+len(pool_scales)*dim, dim,\n",
    "#                       kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(dim),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv2d(num_classes+len(pool_scales)*dim, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Forward is encoder into decoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        \n",
    "        #Referred from https://github.com/CSAILVision/semantic-segmentation-pytorch/blob/e21b8e2bfb1cb145941c36468fc807f20146b71a/models.py\n",
    "#         print(\"After Autoencoder:\",x.shape)\n",
    "        input_size = x.size()\n",
    "        psp_out = [x]\n",
    "        for pool_scale in self.psp:\n",
    "            pool = pool_scale(x)\n",
    "#             print(\"After pooling:\",pool.shape)\n",
    "            p_layer = nn.functional.interpolate(\n",
    "                pool,\n",
    "                (input_size[2], input_size[3]),\n",
    "                mode='bilinear')\n",
    "#             print(\"After upsampling:\",p_layer.shape)\n",
    "            psp_out.append(p_layer)            \n",
    "        x = torch.cat(psp_out, 1)\n",
    "#         print(\"After concatenation:\",x.shape)\n",
    "        x = self.conv_last(x)\n",
    "#         print(\"After last convolution:\",x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MNIST Autoencoder\n",
    "torch.manual_seed(10)\n",
    "model = MNIST_Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in train_loader:\n",
    "#     imgSeq, annotatedOutput, imgName = sample\n",
    "#     model(imgSeq[0].to(device))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Parameters\n",
    "# - MSE Loss, which will be our reconstruction loss for now\n",
    "# - Adam as optimizer\n",
    "# - 20 Epochs\n",
    "# - 128 as batch size\n",
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "  0%|          | 0/9 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input and target batch or spatial sizes don't match: target [8 x 224 x 448], input [8 x 19 x 256 x 448] at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-10c521416244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mannotatedOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotatedOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         output = output.argmax(dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotatedOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input and target batch or spatial sizes don't match: target [8 x 224 x 448], input [8 x 19 x 256 x 448] at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu:23"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "outputs_list = []\n",
    "loss_list = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        imgSeq, annotatedOutput, imgName = data\n",
    "        \n",
    "        # Send data to device\n",
    "        img = Variable(imgSeq[-1])\n",
    "        img = img.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(img)\n",
    "        annotatedOutput = annotatedOutput.to(device).long()\n",
    "#         output = output.argmax(dim=1)\n",
    "        loss = loss_fn(output, annotatedOutput)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Display\n",
    "    print('epoch {}/{}, loss {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    outputs_list.append((epoch, img, output),)\n",
    "    loss_list.append(loss.item())\n",
    "    torch.save(model.state_dict(),\"PSP_Model_epoch_\"+str(epoch)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"psp_model_stoppedhalfwayat13epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "model = MNIST_Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load(\"PSP_Model_epoch_19.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loss\n",
    "plt.figure()\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for k in range(0, 13, 4):\n",
    "    plt.figure(figsize = (100, 50))\n",
    "    imgs = outputs_list[k][1].cpu().detach().numpy()\n",
    "    recon = outputs_list[k][2].cpu().detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1)\n",
    "        print(item.shape)\n",
    "        plt.imshow(item[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertColour(inp, label):\n",
    "    np_inp = inp.numpy()\n",
    "    label = label.numpy()\n",
    "    final = []\n",
    "    for i, i_label in zip(np_inp, label):\n",
    "        out = []\n",
    "        for j, j_label in zip(i, i_label):\n",
    "            if j_label == 255:\n",
    "                out.append(l.trainId2color[j_label])\n",
    "            else:\n",
    "                out.append(l.trainId2color[j])\n",
    "        final.append(out)\n",
    "    return np.asarray(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.labels as l\n",
    "import numpy as np\n",
    "for data in train_loader:\n",
    "\n",
    "    imgSeq, annotatedOutput, imgName = data\n",
    "\n",
    "    # Send data to device\n",
    "    img = Variable(imgSeq[-1])\n",
    "    img = img.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(img)\n",
    "    annotatedOutput = annotatedOutput.to(device).long()\n",
    "    break\n",
    "plt.imshow(convertColour(torch.max(output[0].cpu(),0).indices,annotatedOutput[0].cpu()))\n",
    "print(torch.max(output[0].cpu(),0).indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
